{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, DistilBertModel\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-multilingual-cased\")\n",
    "distilBertModel = DistilBertModel.from_pretrained(\"distilbert/distilbert-base-multilingual-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token               id             \n",
      "-------------------------\n",
      "asya                23795\n",
      "##ingan             30408\n",
      "ליצור               75899\n",
      "Tidligere          101531\n",
      "Júlio              104835\n",
      "этим                37266\n",
      "khi                 12072\n",
      "estiu               59920\n",
      "kullanılır          70408\n",
      "Трудового           89395\n"
     ]
    }
   ],
   "source": [
    "# get some vocab info\n",
    "import random\n",
    "random_tokens = random.sample(list(tokenizer.vocab), 10)\n",
    "random_ids = [tokenizer.vocab[token] for token in random_tokens]\n",
    "\n",
    "print(\"{0:20}{1:15}\".format(\"token\", \"id\"))\n",
    "print(\"-\" * 25)\n",
    "for t, i in zip(random_tokens, random_ids):\n",
    "    print(\"{0:15}{1:10}\".format(t, i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertModel(\n",
       "  (embeddings): Embeddings(\n",
       "    (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (transformer): Transformer(\n",
       "    (layer): ModuleList(\n",
       "      (0-5): 6 x TransformerBlock(\n",
       "        (attention): MultiHeadSelfAttention(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (ffn): FFN(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distilBertModel.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    }
   ],
   "source": [
    "input = tokenizer([\"嗨\", \"hello\"], return_tensors=\"pt\", padding=True, truncation=True)    # Not support batch str input (because of torch.Tensor)\n",
    "seqTensor = distilBertModel(**input, output_hidden_states=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 768])\n"
     ]
    }
   ],
   "source": [
    "print(seqTensor.hidden_states[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "print(len(seqTensor.hidden_states))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,   100,   102,     0],\n",
      "        [  101, 61694, 10133,   102]]), 'attention_mask': tensor([[1, 1, 1, 0],\n",
      "        [1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "print(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set BertDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.DevConf import DevConf\n",
    "devConf = DevConf('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.AttnBlocksConf import AttnBlocksConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.BertDecoder.SentiClassifier import SentiClassifier\n",
    "from utils.const import BlockType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapper = SentiClassifier(AttnBlocksConf(768, 12, 6), BlockType.LAST, devConf=devConf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.9691, -0.9359,  0.6656, -0.0389,  0.0653, -0.1121,  0.5723, -0.7163,\n",
      "         -0.0133,  1.3693, -0.2713,  0.3737, -0.6853, -0.6463, -2.1772,  0.6569,\n",
      "          0.2081, -0.0243, -0.5727, -2.3854,  0.5580,  2.1182, -0.7764,  0.2830,\n",
      "         -0.5762,  1.2624,  1.4614, -0.4876, -0.3879,  0.8895,  0.3116, -0.9578,\n",
      "         -1.5795, -0.5073,  0.8578, -0.1698, -0.3321, -1.2820, -2.0235, -0.4376,\n",
      "          0.6965,  0.3917, -1.3476,  1.0086,  0.8861, -1.1901, -0.5668, -0.5730,\n",
      "         -0.2863, -0.2144,  0.0369, -0.3639, -0.8184, -0.1639, -0.3035, -1.2355,\n",
      "          0.4606, -2.2544, -0.4324,  0.2711, -1.7094, -1.1850, -0.0925,  0.0696,\n",
      "         -1.4869,  0.2667, -0.6693, -1.5943, -1.1482, -0.2062, -1.4222,  0.6467,\n",
      "          0.1574, -0.0290,  2.0798, -0.4745,  0.2181, -0.1663, -1.4687, -0.9155,\n",
      "         -0.4516, -1.0438, -0.8063, -1.7207,  0.7113, -0.2269,  1.1177,  0.8830,\n",
      "          0.5836,  1.4282,  0.0169,  1.6410, -0.9409, -1.4751,  0.6818, -0.1149,\n",
      "         -2.1822, -1.4398,  0.1328, -0.7595, -1.4631,  0.2153,  1.2140,  0.7100,\n",
      "          0.0943,  1.0210,  1.0176, -0.9505, -0.5406,  2.1882, -0.3953,  1.4848,\n",
      "          1.7793,  0.7734, -1.4080, -0.8480, -0.0907,  1.6720,  0.7152,  0.1079,\n",
      "          0.8149, -1.4477,  0.3022, -1.8483, -1.0292,  1.5509, -1.4235, -0.7134,\n",
      "         -0.2040,  0.7637, -0.2081, -0.0377,  1.3165,  0.5192,  0.3255, -0.7234,\n",
      "          0.3912,  0.0167,  0.0429,  0.9650, -0.9478,  0.1316,  1.9792, -1.0498,\n",
      "         -0.0314, -0.0719,  1.2641,  1.9538,  0.2508,  0.1942,  0.9495, -0.4768,\n",
      "          0.6367,  0.3888, -0.3406,  0.0862, -0.5858,  0.3477,  1.0019, -1.5448,\n",
      "          1.0715, -0.1984,  1.1647,  0.6034,  0.2924,  1.1595, -0.3235, -1.0229,\n",
      "         -0.0419,  1.0365,  1.1696, -0.6347,  1.0759,  0.3349,  1.3326, -0.1540,\n",
      "          1.9580,  0.7136, -1.6108, -1.0464, -0.4305,  0.7724,  1.6172,  1.4091,\n",
      "         -0.0080,  0.5421,  0.7136, -1.0641, -0.7025,  0.0895,  0.0224,  0.8861,\n",
      "         -0.6249, -0.3105,  0.2459, -0.1887,  0.2991,  1.0500,  1.1987, -0.1321,\n",
      "         -0.2225, -1.1991, -0.6584,  0.4392, -0.6135,  0.2309,  1.2370,  1.0750,\n",
      "         -0.0085,  1.0553,  1.5716, -0.2972,  2.0276,  0.0685, -0.8848, -0.3118,\n",
      "         -2.3158, -0.1874,  0.3238,  0.8303,  0.3545,  0.9766,  1.4311, -0.3499,\n",
      "         -0.8124,  0.1031, -0.0501,  0.0347,  0.0691,  1.3029, -1.3263, -1.2591,\n",
      "          1.1400,  0.1037,  0.7916, -1.7065, -1.1956, -0.1881,  0.8987,  0.9919,\n",
      "         -0.9744,  0.5467, -1.4724, -1.0195, -0.1178, -1.4045,  2.9894, -1.0708,\n",
      "         -0.1372, -0.7938,  1.5037, -1.7767,  0.7519,  0.8859,  0.1484, -0.9195,\n",
      "         -0.7934, -0.2961,  0.0607,  1.1936, -1.7227, -0.7243, -0.2999,  1.5370,\n",
      "          0.9134, -0.0821, -0.8114, -0.1042, -2.1584, -1.4541, -1.2194, -0.8061,\n",
      "         -0.3955,  1.2612,  1.9545,  0.2669,  1.8414, -0.7101, -0.3579, -0.3250,\n",
      "          0.2418, -0.6382,  0.5916,  0.1415,  1.8945, -1.6825,  0.4336, -0.3513,\n",
      "         -1.0955,  0.1607, -0.9870,  0.1113,  0.8894, -0.1262, -1.5074,  2.1131,\n",
      "         -1.3535,  1.3877, -0.9290,  0.1468,  0.6942,  0.4493, -1.2546,  1.2660,\n",
      "          1.0068,  1.2331, -2.1042,  1.1781, -1.5254,  0.6886, -0.8624,  1.6246,\n",
      "          0.3664, -0.7601,  0.1897, -0.0047,  1.4253,  0.2124, -0.8064,  1.7083,\n",
      "         -0.1104, -1.5976,  0.2012,  0.0045,  1.2435,  1.3967,  0.2450, -0.9893,\n",
      "         -1.6953, -1.4515,  0.2565, -1.0235, -1.9986, -0.4838,  0.4451, -0.4263,\n",
      "         -1.5204,  2.2064,  3.0052,  0.5648, -0.1176,  1.0995,  1.0491,  0.3618,\n",
      "         -0.0185,  1.1801, -0.1228, -1.0068, -1.5565, -0.0471,  0.1561,  0.6685,\n",
      "          0.4781, -0.9274, -0.6550, -1.6561,  0.9399, -0.4825,  1.4005, -0.0444,\n",
      "          0.7917,  0.9927,  1.7971, -0.9362, -0.7268,  0.5734,  1.2100, -0.4946,\n",
      "          0.7477, -1.4993, -1.2376, -0.6627,  1.6681,  1.6719, -0.6394, -0.0648,\n",
      "          1.3255, -0.1761, -1.0489,  1.1122,  1.5061, -1.2645, -0.6558, -0.1872,\n",
      "          0.1081,  0.6089,  0.4993, -1.2654,  0.5086, -1.2878,  1.4882, -0.3109,\n",
      "          0.6637,  0.3586, -1.9818, -1.7926,  0.0841, -1.1565, -2.0286, -0.3358,\n",
      "         -0.4198,  1.4011,  1.4565,  1.1820,  0.3613,  0.6425, -0.0272,  0.6066,\n",
      "         -0.0730,  0.7235,  0.0384,  0.3519,  1.5250, -0.1376,  1.4525, -1.3223,\n",
      "         -0.0571,  0.9494, -0.3546, -1.1099, -2.0172, -0.2783,  1.2872,  0.4164,\n",
      "          0.9285,  0.4116,  0.1962,  1.8520,  0.9896, -0.4038, -0.9480, -0.3285,\n",
      "         -0.7799, -0.2450, -0.6553,  1.3043, -0.3962, -1.4950,  0.1935, -1.0745,\n",
      "         -0.7033, -0.1240,  0.0689,  0.2551,  0.2914,  0.5782, -0.9167, -0.4587,\n",
      "         -1.4346, -0.6687,  1.1240, -0.5857, -0.2040,  0.0713, -1.3293,  0.5709,\n",
      "          0.2568,  1.2363, -1.2624,  1.3110,  0.1689, -0.5456, -0.2395,  0.3876,\n",
      "         -0.9924,  0.0163, -0.3833, -0.0798,  1.1335, -1.2670,  0.8740,  0.4329,\n",
      "          2.1018,  0.6565,  1.7324,  0.0587, -0.9654, -1.4877, -1.8603,  0.6076,\n",
      "          1.7141, -0.3111, -0.7805, -0.3816, -0.4161,  1.5096,  0.8272, -1.1962,\n",
      "          1.4190, -1.0259, -1.2224, -0.4342,  0.0429, -1.8468, -1.4033,  0.3696,\n",
      "         -0.6760, -1.1086, -0.1317,  2.2442, -1.0142, -1.9866, -0.1804, -0.4329,\n",
      "         -0.5007, -1.5116, -0.5172, -0.6653,  0.3401, -0.0873, -1.2962,  0.0069,\n",
      "         -0.5000, -1.1275,  0.9892,  0.7647,  0.1042, -0.3063, -0.9569,  1.0278,\n",
      "          1.0747, -0.5460,  1.1671,  0.7454, -0.5097, -0.3916,  0.3837,  0.2939,\n",
      "         -0.0619, -1.9357, -0.9606, -1.6900, -0.4475,  0.3008,  1.6088, -0.1764,\n",
      "          0.4642,  0.5787, -2.3926,  1.8051, -1.2358, -0.2077,  0.8613, -1.2345,\n",
      "         -0.7145, -0.1913, -0.7108,  0.2379,  1.3958, -0.3756,  1.2955,  2.0119,\n",
      "         -0.4155, -0.3304,  0.4960, -1.2433,  1.5622,  1.2063, -0.7171, -0.0950,\n",
      "          0.3224, -0.9772,  0.9436,  0.8669, -0.0640,  0.1942,  0.9003,  0.3501,\n",
      "         -2.2066, -0.7070,  1.7405, -1.0153,  1.7507,  1.7779, -0.2128,  2.0444,\n",
      "         -1.3438, -0.2059,  0.7271,  0.8571, -0.7254, -1.1924,  1.5850, -0.6884,\n",
      "          1.2660, -0.9532, -0.9332,  1.7934, -1.1132, -1.6990,  0.4017, -1.0302,\n",
      "          0.7436,  0.1539, -0.7786, -0.2032,  0.8108, -0.5138,  0.5956, -0.6375,\n",
      "         -0.2842, -1.1726, -0.9192, -1.0797,  0.4151,  0.3735, -0.0967, -1.1060,\n",
      "          0.2155, -0.4715,  0.8124,  1.8823, -1.3551,  0.3997,  0.5443,  2.2940,\n",
      "         -1.8525, -0.0687,  0.7995, -1.1878, -0.3780,  1.1845,  0.5165,  0.5433,\n",
      "         -0.4600,  0.3654, -0.8471, -0.3045, -0.0423, -1.1194,  0.5099, -0.4667,\n",
      "          1.0753,  1.3657, -0.3219,  0.0912,  0.4823, -0.5224,  1.4483,  0.6461,\n",
      "         -0.7356, -0.2629, -1.3524, -0.4956, -1.3187, -0.5447,  0.3619,  1.4628,\n",
      "          0.4277,  3.4171, -1.1573, -1.0308, -1.3734,  1.2483,  0.0803,  0.3653,\n",
      "          1.2514,  0.1938,  1.1712, -0.1903, -0.9054, -0.7937,  1.4227, -0.2607,\n",
      "          0.0483, -0.6385,  0.5947, -0.7952,  0.0456, -1.6272, -0.5469, -0.9472,\n",
      "          0.3680, -0.5064,  0.2830,  0.8808,  1.3879,  1.2495,  0.7785, -0.1209,\n",
      "         -0.1466,  0.8383, -0.6716,  0.9283,  1.1115,  0.7826, -1.4402,  1.3152,\n",
      "         -1.3058,  0.0591,  1.1058,  0.7436,  0.0718,  0.7410, -1.4890, -0.4004,\n",
      "          1.8021,  0.1885,  0.0076,  1.2833, -0.7955,  0.5108,  0.8408,  0.2184,\n",
      "          0.8075,  0.6524, -0.4029, -0.3957,  1.3433, -0.2092, -0.8455, -1.3611,\n",
      "          0.1841, -0.8989, -0.0391, -1.5676, -1.1910,  0.2066,  0.4068,  0.1004,\n",
      "          0.6041,  1.1341, -1.6763, -0.6598,  0.3279,  0.1362,  0.5346,  0.9597,\n",
      "          0.3002, -1.5899,  1.1686, -1.8286, -0.4594,  2.1726, -0.0105,  0.0887,\n",
      "          1.0292,  1.4612,  0.0982,  1.2139, -1.2383, -0.2326, -0.1080,  0.4947,\n",
      "          0.2681, -0.8064, -0.4000,  0.6682,  1.5456, -1.7308, -0.1304,  0.8174,\n",
      "         -0.4531, -0.8723,  0.9549,  0.7369, -1.0479, -0.5754,  0.1096, -0.2649,\n",
      "          0.2872,  0.4386, -0.7282, -0.5937, -0.2716,  0.9838, -1.2768, -0.4996]],\n",
      "       device='mps:0', grad_fn=<SqueezeBackward1>)\n"
     ]
    }
   ],
   "source": [
    "ans = mapper.forward(seqTensor)\n",
    "# print(ans.shape)\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 768])\n"
     ]
    }
   ],
   "source": [
    "print(ans.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combination Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.CombinationModel import CombinationModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "cModel = CombinationModel(\n",
    "    # tokenizer=tokenizer,\n",
    "    distilBert=distilBertModel,\n",
    "    decoder=mapper,\n",
    "    outputProject=nn.Linear(768, 5, device=devConf.device, dtype=devConf.dtype)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.0764, 0.3002, 0.1742, 0.1757, 0.2736]], grad_fn=<SoftmaxBackward0>),\n",
       " tensor([[[0.3497, 0.3305, 0.3199]]], grad_fn=<MeanBackward1>))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cModel.forward(**tokenizer(\"嗨\", return_tensors=\"pt\"), returnAttnWeight=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.datasets import IMDB\n",
    "from torchtext.data.functional import to_map_style_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData = iter(IMDB(root=\"./data\", split='train'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.<br /><br />What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it's not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.<br /><br />I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn't have much of a plot.\n"
     ]
    }
   ],
   "source": [
    "print(next(trainData)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch)->tuple[Tensor, Tensor, Tensor]:\n",
    "    target =[]\n",
    "    datas = []\n",
    "\n",
    "    for (label, data) in batch:\n",
    "        datas.append(data)\n",
    "        target.append(0 if label == 'neg' else 1)\n",
    "\n",
    "    encoding = tokenizer(datas, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "    ids = encoding['input_ids']\n",
    "    mask = encoding['attention_mask']\n",
    "    \n",
    "    return ids, mask, torch.tensor(target, dtype=torch.int16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainLoader = DataLoader(to_map_style_dataset(trainData), collate_fn=collate_fn, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[  101, 10747, 18379,  ...,     0,     0,     0],\n",
       "         [  101, 17443, 49307,  ..., 10833, 10124,   102],\n",
       "         [  101, 76783, 10230,  ...,     0,     0,     0],\n",
       "         ...,\n",
       "         [  101,   146, 10134,  ...,     0,     0,     0],\n",
       "         [  101, 14027, 18379,  ...,     0,     0,     0],\n",
       "         [  101, 12689,   169,  ...,   118, 18923,   102]]),\n",
       " tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 1, 1, 1]]),\n",
       " tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int16))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(trainLoader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## deprecated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.utils import get_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer('basic_english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'rented', 'i', 'am', 'curious-yellow', 'from', 'my', 'video', 'store', 'because', 'of', 'all', 'the', 'controversy', 'that', 'surrounded', 'it', 'when', 'it', 'was', 'first', 'released', 'in', '1967', '.', 'i', 'also', 'heard', 'that', 'at', 'first', 'it', 'was', 'seized', 'by', 'u', '.', 's', '.', 'customs', 'if', 'it', 'ever', 'tried', 'to', 'enter', 'this', 'country', ',', 'therefore', 'being', 'a', 'fan', 'of', 'films', 'considered', 'controversial', 'i', 'really', 'had', 'to', 'see', 'this', 'for', 'myself', '.', 'the', 'plot', 'is', 'centered', 'around', 'a', 'young', 'swedish', 'drama', 'student', 'named', 'lena', 'who', 'wants', 'to', 'learn', 'everything', 'she', 'can', 'about', 'life', '.', 'in', 'particular', 'she', 'wants', 'to', 'focus', 'her', 'attentions', 'to', 'making', 'some', 'sort', 'of', 'documentary', 'on', 'what', 'the', 'average', 'swede', 'thought', 'about', 'certain', 'political', 'issues', 'such', 'as', 'the', 'vietnam', 'war', 'and', 'race', 'issues', 'in', 'the', 'united', 'states', '.', 'in', 'between', 'asking', 'politicians', 'and', 'ordinary', 'denizens', 'of', 'stockholm', 'about', 'their', 'opinions', 'on', 'politics', ',', 'she', 'has', 'sex', 'with', 'her', 'drama', 'teacher', ',', 'classmates', ',', 'and', 'married', 'men', '.', 'what', 'kills', 'me', 'about', 'i', 'am', 'curious-yellow', 'is', 'that', '40', 'years', 'ago', ',', 'this', 'was', 'considered', 'pornographic', '.', 'really', ',', 'the', 'sex', 'and', 'nudity', 'scenes', 'are', 'few', 'and', 'far', 'between', ',', 'even', 'then', 'it', \"'\", 's', 'not', 'shot', 'like', 'some', 'cheaply', 'made', 'porno', '.', 'while', 'my', 'countrymen', 'mind', 'find', 'it', 'shocking', ',', 'in', 'reality', 'sex', 'and', 'nudity', 'are', 'a', 'major', 'staple', 'in', 'swedish', 'cinema', '.', 'even', 'ingmar', 'bergman', ',', 'arguably', 'their', 'answer', 'to', 'good', 'old', 'boy', 'john', 'ford', ',', 'had', 'sex', 'scenes', 'in', 'his', 'films', '.', 'i', 'do', 'commend', 'the', 'filmmakers', 'for', 'the', 'fact', 'that', 'any', 'sex', 'shown', 'in', 'the', 'film', 'is', 'shown', 'for', 'artistic', 'purposes', 'rather', 'than', 'just', 'to', 'shock', 'people', 'and', 'make', 'money', 'to', 'be', 'shown', 'in', 'pornographic', 'theaters', 'in', 'america', '.', 'i', 'am', 'curious-yellow', 'is', 'a', 'good', 'film', 'for', 'anyone', 'wanting', 'to', 'study', 'the', 'meat', 'and', 'potatoes', '(', 'no', 'pun', 'intended', ')', 'of', 'swedish', 'cinema', '.', 'but', 'really', ',', 'this', 'film', 'doesn', \"'\", 't', 'have', 'much', 'of', 'a', 'plot', '.']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer(next(trainData)[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentiClassifier(distilBertModel=distilBertModel, tokenizer=tokenizer, output_dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4752, 0.5248]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(model(\"hello world!\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from einops import repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.ones(2, requires_grad=True)\n",
    "y = repeat(x, \"l -> b l\", b=2) * 2\n",
    "# with torch.no_grad():\n",
    "#     for i in range(3):\n",
    "#         y = 2*y\n",
    "#     # x.retain_grad()\n",
    "y1 = 2*y[0]\n",
    "y2 = y[1]\n",
    "z = y1.sum() + y2.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1.], requires_grad=True)\n",
      "tensor([[4., 4.],\n",
      "        [4., 4.]], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([6., 6.])\n"
     ]
    }
   ],
   "source": [
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a b c d e \n"
     ]
    }
   ],
   "source": [
    "strlist = [\"a\", \"b\", \"c\", \"d\", \"e\"]\n",
    "concatStr = \"\"\n",
    "for i in strlist:\n",
    "    concatStr+=i\n",
    "    concatStr+=\" \"\n",
    "print(concatStr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "expected sequence of length 2 at dim 1 (got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m loss_fn(torch\u001b[38;5;241m.\u001b[39mtensor([[\u001b[38;5;241m0.1\u001b[39m, \u001b[38;5;241m0.9\u001b[39m, \u001b[38;5;241m0.2\u001b[39m], [\u001b[38;5;241m0.1\u001b[39m, \u001b[38;5;241m0.9\u001b[39m, \u001b[38;5;241m0.5\u001b[39m]]), \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m) \n",
      "\u001b[1;31mValueError\u001b[0m: expected sequence of length 2 at dim 1 (got 1)"
     ]
    }
   ],
   "source": [
    "loss_fn(torch.tensor([[0.1, 0.9, 0.2], [0.1, 0.9, 0.5]]), torch.tensor([[1, 2], [1]])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
