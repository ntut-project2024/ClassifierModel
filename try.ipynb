{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, DistilBertModel\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-multilingual-cased\")\n",
    "distilBertModel = DistilBertModel.from_pretrained(\"distilbert/distilbert-base-multilingual-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get some vocab info\n",
    "import random\n",
    "random_tokens = random.sample(list(tokenizer.vocab), 10)\n",
    "random_ids = [tokenizer.vocab[token] for token in random_tokens]\n",
    "\n",
    "print(\"{0:20}{1:15}\".format(\"token\", \"id\"))\n",
    "print(\"-\" * 25)\n",
    "for t, i in zip(random_tokens, random_ids):\n",
    "    print(\"{0:15}{1:10}\".format(t, i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertModel(\n",
       "  (embeddings): Embeddings(\n",
       "    (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (transformer): Transformer(\n",
       "    (layer): ModuleList(\n",
       "      (0-5): 6 x TransformerBlock(\n",
       "        (attention): MultiHeadSelfAttention(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (ffn): FFN(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distilBertModel.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = tokenizer(\"嗨\", return_tensors=\"pt\")\n",
    "seqTensor = distilBertModel(**input, output_hidden_states=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 768])\n"
     ]
    }
   ],
   "source": [
    "print(seqTensor.hidden_states[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "print(len(seqTensor.hidden_states))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[101, 100, 102]]), 'attention_mask': tensor([[1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "print(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set BertDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.DevConf import DevConf\n",
    "devConf = DevConf('mps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.BertDecoder.SentiClassifier import SentiClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapper = SentiClassifier(768, 12, 2, devConf=devConf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.BertDecoder.SentiClassifier_Cross import SentiClassifier_Cross"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(not isinstance(mapper, SentiClassifier))\n",
    "print(not isinstance(mapper, SentiClassifier_Cross))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.5515e-02,  6.7839e-02, -2.8733e-02, -7.7939e-02, -4.8201e-02,\n",
      "          7.6650e-04,  6.0756e-02,  1.4687e-02, -3.5091e-02,  3.8786e-02,\n",
      "         -2.4219e-02,  6.9973e-02, -3.5821e-03,  1.1017e-01, -6.1632e-02,\n",
      "          3.2076e-03,  1.7479e-02,  4.2271e-02,  5.5858e-02, -4.7596e-02,\n",
      "         -1.0977e-01, -4.5340e-03,  3.8417e-02,  7.4270e-02, -1.0639e-01,\n",
      "          1.1155e-01,  8.8370e-02,  4.1836e-04, -1.7464e-03, -1.2006e-02,\n",
      "          1.2026e-01,  5.8393e-02, -4.9086e-02,  1.1670e-01, -2.2997e-02,\n",
      "         -2.1828e-02, -1.8341e-02, -1.1862e-01,  9.6593e-02,  1.4285e-01,\n",
      "         -1.0608e-01, -2.8651e-02, -7.1754e-02,  3.7221e-02,  3.7768e-02,\n",
      "         -2.5128e-02,  2.0181e-03,  1.6639e-02, -3.0742e-02,  5.3533e-02,\n",
      "         -5.7006e-02, -4.9872e-02, -3.6237e-03, -1.4313e-01, -1.7708e-02,\n",
      "         -6.0654e-02, -4.4661e-02,  2.1638e-02,  1.3119e-03,  1.0348e-02,\n",
      "          7.9516e-03,  7.5657e-02,  2.5445e-02,  3.4548e-02,  2.6398e-02,\n",
      "          3.6327e-02,  2.2170e-03,  2.6830e-02, -2.4782e-02,  7.9772e-02,\n",
      "         -3.0407e-02,  5.7797e-03, -8.9780e-03,  3.9784e-02,  1.8838e-02,\n",
      "          1.4231e-02,  1.8362e-03, -1.0483e-02,  1.0772e-01, -1.9717e-02,\n",
      "          4.7832e-02,  3.6391e-02,  2.7113e-02,  6.2909e-02, -1.8576e-02,\n",
      "          2.4861e-02,  6.2787e-02,  6.0550e-02,  1.5341e-01,  1.5181e-02,\n",
      "         -3.1380e-02,  7.8272e-02,  1.2321e-02,  5.6419e-02,  5.9987e-02,\n",
      "          1.4181e-02,  3.9304e-02, -4.3132e-02, -7.8523e-02, -5.8677e-02,\n",
      "          5.1777e-02,  1.2351e-02, -5.3168e-02, -7.8664e-02,  3.3015e-02,\n",
      "          1.0973e-02, -1.1637e-01,  3.0540e-02,  9.2093e-02,  3.4554e-02,\n",
      "         -4.6400e-02,  2.0837e-02,  1.1249e-02, -7.4058e-02,  1.0954e-02,\n",
      "         -6.4823e-02,  1.0944e-01, -6.4390e-02, -1.7477e-01,  8.5393e-03,\n",
      "         -2.7523e-02,  8.5638e-02, -3.4860e-02, -4.0999e-02, -3.0354e-02,\n",
      "          3.8182e-02, -2.1758e-02,  2.8434e-02, -1.9590e-02,  2.9104e-02,\n",
      "          3.6163e-02, -6.3589e-02,  4.0340e-02, -4.0967e-02,  2.1039e-02,\n",
      "          6.3882e-02, -2.6050e-02,  6.4534e-02,  2.2446e-02, -1.4635e-01,\n",
      "          3.0245e-02, -2.0210e-02, -5.9120e-02,  5.0817e-02, -1.9207e-02,\n",
      "         -7.6811e-02, -3.0079e-02,  6.9605e-02,  8.0057e-02,  9.5221e-02,\n",
      "         -9.7182e-03,  3.2284e-02,  6.0750e-02, -1.2798e-01,  4.0045e-04,\n",
      "          7.2617e-02, -1.0168e-01,  1.1837e-02, -2.3439e-02,  3.1520e-02,\n",
      "         -4.3855e-02,  5.4812e-02,  5.2040e-03,  1.3271e-02, -5.8317e-02,\n",
      "         -7.3572e-02, -5.3135e-02, -1.7879e-02,  6.3456e-02, -1.0345e-01,\n",
      "          1.4270e-01,  7.0973e-02, -3.8674e-02,  5.3006e-02, -4.4043e-02,\n",
      "          1.9241e-02, -5.6048e-02,  9.8843e-02, -3.5614e-02,  3.1768e-02,\n",
      "          2.3934e-02,  8.0119e-02,  2.0970e-02,  4.2472e-02, -4.3410e-02,\n",
      "         -7.8549e-03,  1.3685e-01,  2.5043e-02,  1.1794e-02, -9.8028e-02,\n",
      "         -1.2173e-01, -2.3772e-02, -1.3524e-02,  5.2447e-03, -1.8144e-02,\n",
      "         -8.9963e-02,  5.1495e-02, -1.0922e-01, -6.7519e-02, -1.0254e-01,\n",
      "          5.7653e-02,  5.2349e-02,  3.0874e-03,  2.7673e-02,  8.8686e-02,\n",
      "          1.3277e-02,  1.5108e-02,  1.7640e-01, -7.8806e-02,  5.1122e-02,\n",
      "          3.8147e-02,  1.1014e-01,  5.1075e-02, -6.2547e-02,  4.8800e-02,\n",
      "         -5.6188e-02, -9.5476e-03,  4.0867e-02, -1.5276e-01, -1.0373e-02,\n",
      "         -5.2867e-02,  3.9280e-02, -5.2721e-02, -1.3589e-02,  2.1535e-02,\n",
      "         -1.7605e-02, -1.3999e-02,  2.5854e-02,  1.0920e-01, -2.5840e-02,\n",
      "          6.3765e-02,  9.1947e-02, -1.3594e-02,  9.0429e-03, -2.6898e-02,\n",
      "         -2.9818e-02,  3.0806e-02, -1.5233e-02, -1.1098e-01,  1.0813e-01,\n",
      "         -5.5320e-02, -7.1823e-02, -5.6410e-02, -4.6919e-02,  1.2630e-02,\n",
      "         -8.9230e-03, -2.3002e-02, -2.0002e-02,  7.6510e-02,  1.0134e-01,\n",
      "         -2.3056e-03, -1.5389e-02, -5.3563e-03, -5.4785e-02, -9.6622e-04,\n",
      "          9.8580e-03, -8.5603e-02,  1.3562e-02,  9.8927e-02,  6.9121e-02,\n",
      "         -4.8592e-02,  2.1265e-02, -3.7714e-02,  2.3279e-02, -5.9725e-02,\n",
      "          1.0734e-01, -5.8245e-02, -5.3680e-02,  3.8470e-02,  5.0215e-02,\n",
      "          8.0767e-02, -1.6362e-02, -9.0405e-03,  3.1759e-02, -2.5574e-03,\n",
      "          1.9790e-03,  3.1513e-02,  8.7723e-03, -2.6742e-03,  1.4795e-02,\n",
      "         -1.8767e-02,  4.2093e-02, -1.4505e-02, -9.0924e-02, -3.3460e-02,\n",
      "         -2.9672e-02, -3.8453e-02, -3.2151e-02,  9.2779e-02,  2.8558e-02,\n",
      "         -1.1157e-03, -4.4784e-02,  4.6845e-02,  2.7986e-02,  1.2079e-01,\n",
      "         -1.2432e-02,  8.8423e-03, -8.0100e-02, -1.1345e-02, -1.9856e-03,\n",
      "          1.2097e-01,  7.0360e-02,  3.4054e-02, -7.3163e-02,  1.0857e-02,\n",
      "         -1.0103e-02,  1.5912e-02, -5.4645e-03, -8.1321e-02, -6.5920e-02,\n",
      "         -2.9328e-02,  8.0144e-03,  6.1907e-03, -1.2484e-02,  8.3786e-02,\n",
      "          5.6786e-02,  3.7400e-02, -8.0377e-02,  5.6812e-03, -1.0457e-01,\n",
      "          1.3933e-01,  8.7036e-02,  1.3933e-01,  8.4820e-02,  7.0494e-02,\n",
      "         -5.2693e-03,  2.5440e-02, -5.6901e-02, -1.1961e-01,  2.4070e-02,\n",
      "          2.3971e-02,  2.0790e-02, -1.6390e-03, -8.5848e-02,  9.4193e-02,\n",
      "          5.3004e-02, -6.5512e-02, -5.3849e-02, -3.9726e-02, -3.9094e-02,\n",
      "         -4.8738e-03, -1.2415e-01, -1.5738e-03,  1.2024e-01, -2.6842e-02,\n",
      "          1.0647e-01, -4.3922e-02, -3.6443e-02,  4.1147e-02, -7.5580e-02,\n",
      "          7.5635e-02, -3.5372e-02, -1.0566e-02, -6.1786e-02, -7.9519e-02,\n",
      "         -8.0862e-02,  1.7194e-02,  1.1527e-03,  1.8052e-02, -2.6517e-02,\n",
      "          7.6701e-02,  7.3453e-04,  1.4776e-02, -5.0517e-02,  1.3782e-01,\n",
      "          6.4583e-02, -2.3253e-02, -1.2395e-01, -7.5113e-03,  1.1901e-02,\n",
      "          3.4133e-02,  3.5207e-02, -4.5884e-02, -6.3662e-02, -8.1191e-02,\n",
      "         -5.2302e-02, -1.9125e-02, -3.2763e-02,  3.3069e-02, -1.6204e-02,\n",
      "          9.9725e-02, -3.3546e-02, -1.6081e-02, -2.6951e-02,  4.0199e-02,\n",
      "         -4.3954e-02,  1.1299e-01,  1.2553e-02,  6.7418e-02, -1.1133e-02,\n",
      "          2.3217e-02,  7.9480e-02, -2.2641e-02,  1.7442e-02,  2.3312e-02,\n",
      "          1.4970e-02, -2.5485e-02, -4.1260e-02, -1.7492e-02,  6.5290e-02,\n",
      "          1.9247e-02,  1.2121e-01,  4.5348e-02, -4.7015e-02,  5.5386e-02,\n",
      "         -7.5764e-02, -4.4630e-02,  6.9488e-02,  1.5107e-01, -5.5561e-02,\n",
      "         -1.4895e-02, -4.3475e-02,  1.0334e-01,  1.5220e-03,  1.2458e-01,\n",
      "          1.6518e-03, -2.7823e-02, -7.2005e-02, -1.2133e-02, -1.7895e-02,\n",
      "          4.6080e-02,  3.1740e-02, -6.1456e-02, -2.8698e-02, -4.3712e-02,\n",
      "         -5.9651e-02,  1.9327e-02, -1.1336e-02,  3.1036e-02,  6.9539e-02,\n",
      "         -5.8819e-02,  6.9721e-02, -4.0279e-02,  5.6530e-02, -5.6433e-02,\n",
      "         -1.0979e-01, -1.0143e-01, -2.1585e-02, -2.6018e-02,  3.8312e-02,\n",
      "          5.6782e-02, -1.7814e-02,  1.9185e-02, -2.8632e-02,  7.5531e-04,\n",
      "          5.2761e-02,  6.6264e-02, -1.3974e-02, -6.6126e-02, -5.3092e-02,\n",
      "         -4.1812e-02,  2.4042e-02, -7.7402e-02, -9.3508e-02, -2.9330e-02,\n",
      "          7.5326e-02, -4.2495e-02, -8.1162e-02,  3.8283e-02,  5.1676e-02,\n",
      "         -6.2085e-02,  2.5663e-02, -8.2694e-02,  4.5699e-02, -8.7490e-02,\n",
      "          1.6048e-02,  5.0215e-03,  1.1688e-02, -4.0405e-02,  3.5629e-02,\n",
      "          8.8420e-02, -6.9244e-02,  5.4703e-02,  4.4138e-02, -6.2233e-02,\n",
      "         -7.4078e-03, -6.4747e-02,  5.9430e-02, -9.6170e-03,  8.5280e-02,\n",
      "         -2.1460e-02, -3.2004e-02, -8.5851e-02,  4.4224e-02, -1.2422e-01,\n",
      "         -3.1798e-02, -2.5117e-02,  8.5167e-02, -1.2891e-02,  2.3707e-02,\n",
      "          8.0436e-02, -1.3451e-01, -7.4157e-03, -3.2097e-02, -7.9377e-02,\n",
      "          3.6259e-02,  7.3990e-02,  1.3336e-02, -4.1919e-02, -7.4129e-02,\n",
      "          4.2684e-02,  8.2684e-02,  1.4419e-02, -7.3505e-02, -3.0201e-02,\n",
      "          2.5985e-02,  7.0620e-02, -4.8180e-02,  6.4186e-02, -1.3015e-01,\n",
      "         -4.5444e-02, -1.5939e-02, -1.0932e-01,  6.9750e-02,  7.2368e-02,\n",
      "         -9.1149e-02, -1.1691e-01, -1.1527e-01, -4.9316e-02,  1.2377e-02,\n",
      "         -1.0194e-01, -7.4657e-02,  4.2123e-03,  1.1499e-02, -1.0381e-02,\n",
      "         -1.2298e-01, -8.1628e-02, -4.4799e-02, -4.0697e-02, -1.2402e-02,\n",
      "         -3.7693e-02,  6.8662e-02, -6.6908e-02, -2.7189e-02, -2.2898e-02,\n",
      "         -1.1810e-02, -1.0666e-01,  4.7947e-02, -7.1574e-02, -1.0372e-01,\n",
      "         -1.5036e-01,  7.5547e-02, -4.2701e-02, -4.0335e-02,  1.9151e-03,\n",
      "         -8.2052e-02, -8.1438e-04, -9.5767e-02, -6.6244e-02, -9.6988e-02,\n",
      "         -2.2068e-02,  6.7154e-02, -6.4565e-03, -6.4413e-02,  6.3531e-02,\n",
      "         -5.7140e-02, -4.7355e-02, -5.3983e-02, -9.2452e-02, -1.6363e-02,\n",
      "         -2.9543e-02,  3.1113e-02, -4.3576e-02, -1.5359e-02, -3.9776e-02,\n",
      "          1.4467e-03, -1.4910e-02,  4.1104e-03,  7.5404e-02,  1.1230e-01,\n",
      "         -6.0309e-02,  6.5351e-02,  9.4163e-02,  3.9084e-03, -2.9969e-02,\n",
      "          5.7379e-02, -4.0673e-02,  2.4917e-02, -6.2920e-02, -4.0783e-03,\n",
      "         -1.2317e-02,  4.5449e-03, -2.2638e-02, -3.7828e-02,  3.6575e-04,\n",
      "          3.6340e-02, -2.7588e-02, -4.2848e-02, -4.7284e-02, -3.3049e-04,\n",
      "         -4.7338e-02, -4.6591e-03,  9.8017e-03,  1.2522e-01, -2.9463e-02,\n",
      "         -7.8671e-02, -7.5608e-03, -7.8302e-02, -5.0561e-02, -4.0810e-02,\n",
      "         -6.2553e-02, -3.3199e-02, -1.8204e-03,  7.2772e-03,  1.9817e-02,\n",
      "          8.1429e-02, -5.8024e-02,  1.0930e-01,  1.8147e-01, -9.5423e-02,\n",
      "          4.4050e-02, -3.6076e-02, -4.9123e-04,  2.1743e-02, -6.9805e-02,\n",
      "          4.2761e-02,  2.1110e-02,  1.2421e-02, -1.0744e-02, -7.8550e-02,\n",
      "          1.1046e-02,  1.7968e-02, -3.2225e-02,  2.8885e-02, -9.3795e-03,\n",
      "          1.9760e-02, -6.6049e-04, -3.0089e-02,  2.9200e-02,  5.8787e-02,\n",
      "          8.8418e-02,  4.0001e-02, -1.1579e-01, -1.1967e-03, -1.6177e-02,\n",
      "          2.3364e-02,  3.7455e-02, -6.9168e-02,  4.1795e-02,  1.2245e-02,\n",
      "          5.8927e-02, -4.9618e-02, -1.6872e-01,  7.4567e-02,  3.6915e-02,\n",
      "         -1.0077e-01, -2.7684e-03,  1.5731e-01,  8.2396e-03, -1.3854e-01,\n",
      "          3.1786e-02,  9.1609e-03, -2.1176e-02,  1.1174e-05,  4.0560e-02,\n",
      "          3.0520e-03, -6.7324e-02, -3.9818e-02, -1.8351e-01,  3.6258e-02,\n",
      "          3.2811e-02, -6.8634e-02, -9.5564e-03, -3.9170e-02,  7.3423e-02,\n",
      "         -5.5325e-02, -2.7562e-02,  4.6012e-02, -4.8929e-03,  2.5699e-02,\n",
      "          6.2754e-02, -3.6139e-02,  1.7746e-02, -4.7678e-02, -1.1918e-02,\n",
      "         -2.6900e-02, -2.4052e-02, -3.9694e-02,  3.7869e-02, -4.1003e-02,\n",
      "         -1.9687e-02,  2.4707e-02, -2.8959e-02, -1.2835e-02,  7.2304e-02,\n",
      "         -5.3692e-02,  1.1655e-02,  3.9476e-02, -8.8540e-02,  9.8878e-02,\n",
      "          1.0963e-02, -1.0955e-02,  1.3855e-02,  3.6131e-02,  1.0963e-02,\n",
      "          1.1360e-01, -1.2059e-02, -9.0855e-02, -2.1973e-02, -6.4618e-02,\n",
      "          6.0705e-02, -5.4682e-02,  4.9087e-02,  5.0522e-03,  6.4949e-02,\n",
      "         -9.0971e-03,  5.5658e-02, -2.2907e-02, -1.6581e-03,  2.3357e-02,\n",
      "         -1.8036e-01,  2.6753e-02,  7.1959e-03,  1.9766e-02,  9.6205e-02,\n",
      "          2.5808e-02,  9.1513e-02, -1.3151e-01, -8.4036e-03, -5.5065e-02,\n",
      "         -6.3341e-02, -2.6632e-02, -4.0170e-02, -1.6294e-02, -5.7100e-02,\n",
      "          2.1696e-02, -5.5690e-02,  5.0791e-02, -3.7115e-02, -9.8982e-03,\n",
      "          8.0586e-02, -4.1621e-03, -4.1289e-03, -1.5895e-02, -1.2481e-01,\n",
      "          2.2993e-02, -2.0352e-02, -4.8336e-02,  2.5599e-02, -8.0113e-02,\n",
      "         -2.4921e-02,  7.9667e-03, -1.3521e-02,  1.9309e-02,  3.7910e-02,\n",
      "         -2.7098e-02,  2.3670e-02,  1.1152e-01,  5.2154e-02, -1.4102e-01,\n",
      "          5.2562e-02,  5.8007e-02, -4.3789e-02,  4.2451e-02,  4.5202e-02,\n",
      "          1.5059e-01, -2.1797e-03,  6.8531e-02, -4.0734e-02, -7.8771e-02,\n",
      "         -2.6741e-02,  2.3061e-02, -1.4414e-01, -1.8644e-01,  4.2295e-02,\n",
      "          8.4401e-02, -6.5154e-02, -1.1769e-02]], device='mps:0',\n",
      "       grad_fn=<SqueezeBackward1>)\n"
     ]
    }
   ],
   "source": [
    "ans = mapper.forward(seqTensor)\n",
    "# print(ans.shape)\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 768])\n"
     ]
    }
   ],
   "source": [
    "print(ans.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combination Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.CombinationModel import CombinationModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cModel = CombinationModel(tokenizer=tokenizer, distilBert=distilBertModel, decoder=mapper, outputProject=nn.Linear(768, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mcModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m嗨\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Volumes/ASUS FX/ai/Bert/model/CombinationModel.py:41\u001b[0m, in \u001b[0;36mCombinationModel.forward\u001b[0;34m(self, input, NoGradBert, NoGradDecoder, returnAttnWeight)\u001b[0m\n\u001b[1;32m     39\u001b[0m         output, attnWeig \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(output, returnAttnWeight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 41\u001b[0m     output, attnWeig \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturnAttnWeight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m returnAttnWeight:\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m nn\u001b[38;5;241m.\u001b[39mSoftmax(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutProj(output)), attnWeig\n",
      "File \u001b[0;32m/Volumes/ASUS FX/ai/Bert/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Volumes/ASUS FX/ai/Bert/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/Volumes/ASUS FX/ai/Bert/model/BertDecoder/SentiClassifier.py:31\u001b[0m, in \u001b[0;36mSentiClassifier.forward\u001b[0;34m(self, input, returnAttnWeight)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     25\u001b[0m         \u001b[38;5;28minput\u001b[39m: BaseModelOutput,\n\u001b[1;32m     26\u001b[0m         returnAttnWeight: \u001b[38;5;28mbool\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     27\u001b[0m     )\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m\u001b[38;5;28mtuple\u001b[39m[Tensor, Tensor] \u001b[38;5;241m|\u001b[39m Tensor:\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mlast_hidden_state\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevConf\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m---> 31\u001b[0m     sentVec, attnWeight \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmapper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Q\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mneed_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m returnAttnWeight:\n\u001b[1;32m     34\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m sentVec\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m), attnWeight\n",
      "File \u001b[0;32m/Volumes/ASUS FX/ai/Bert/utils/MHABlocks.py:38\u001b[0m, in \u001b[0;36mMHABlocks.forward\u001b[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m     27\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     28\u001b[0m         query: Tensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     35\u001b[0m         is_causal : \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     36\u001b[0m     )\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m\u001b[38;5;28mtuple\u001b[39m[Tensor, Optional[Tensor]]:\n\u001b[0;32m---> 38\u001b[0m     query, attnWeight \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mha\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_layerNum\u001b[49m\u001b[43m]\u001b[49m(\n\u001b[1;32m     39\u001b[0m             query\u001b[38;5;241m=\u001b[39mquery,\n\u001b[1;32m     40\u001b[0m             key\u001b[38;5;241m=\u001b[39mkey,\n\u001b[1;32m     41\u001b[0m             value\u001b[38;5;241m=\u001b[39mvalue,\n\u001b[1;32m     42\u001b[0m             key_padding_mask\u001b[38;5;241m=\u001b[39mkey_padding_mask,\n\u001b[1;32m     43\u001b[0m             need_weights\u001b[38;5;241m=\u001b[39mneed_weights,\n\u001b[1;32m     44\u001b[0m             attn_mask\u001b[38;5;241m=\u001b[39mattn_mask,\n\u001b[1;32m     45\u001b[0m             average_attn_weights\u001b[38;5;241m=\u001b[39maverage_attn_weights,\n\u001b[1;32m     46\u001b[0m             is_causal\u001b[38;5;241m=\u001b[39mis_causal)\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_layerNum):\n\u001b[1;32m     49\u001b[0m         query \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mha[i]\u001b[38;5;241m.\u001b[39mforward(\n\u001b[1;32m     50\u001b[0m             query\u001b[38;5;241m=\u001b[39mquery,\n\u001b[1;32m     51\u001b[0m             key\u001b[38;5;241m=\u001b[39mkey,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     56\u001b[0m             average_attn_weights\u001b[38;5;241m=\u001b[39maverage_attn_weights,\n\u001b[1;32m     57\u001b[0m             is_causal\u001b[38;5;241m=\u001b[39mis_causal)\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "cModel.forward(\"嗨\", True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentiClassifier(distilBertModel=distilBertModel, tokenizer=tokenizer, output_dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4752, 0.5248]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(model(\"hello world!\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from einops import repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.ones(2, requires_grad=True)\n",
    "y = repeat(x, \"l -> b l\", b=2) * 2\n",
    "# with torch.no_grad():\n",
    "#     for i in range(3):\n",
    "#         y = 2*y\n",
    "#     # x.retain_grad()\n",
    "y1 = 2*y[0]\n",
    "y2 = y[1]\n",
    "z = y1.sum() + y2.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1.], requires_grad=True)\n",
      "tensor([[4., 4.],\n",
      "        [4., 4.]], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "z.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([6., 6.])\n"
     ]
    }
   ],
   "source": [
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
