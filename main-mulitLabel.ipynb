{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '_C' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mDevConf\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DevConf\n\u001b[0;32m      2\u001b[0m DEV_CONF \u001b[38;5;241m=\u001b[39m DevConf(device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32md:\\code\\python\\bert\\utils\\DevConf.py:2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdataclasses\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dataclass\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;129m@dataclass\u001b[39m(slots\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mDevConf\u001b[39;00m:\n\u001b[0;32m      6\u001b[0m     device: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[1;32md:\\code\\python\\bert\\.venv\\Lib\\site-packages\\torch\\__init__.py:560\u001b[0m\n\u001b[0;32m    546\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(textwrap\u001b[38;5;241m.\u001b[39mdedent(\u001b[38;5;124m'''\u001b[39m\n\u001b[0;32m    547\u001b[0m \u001b[38;5;124m            Failed to load PyTorch C extensions:\u001b[39m\n\u001b[0;32m    548\u001b[0m \u001b[38;5;124m                It appears that PyTorch has loaded the `torch/_C` folder\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    556\u001b[0m \u001b[38;5;124m                or by running Python from a different directory.\u001b[39m\n\u001b[0;32m    557\u001b[0m \u001b[38;5;124m            \u001b[39m\u001b[38;5;124m'''\u001b[39m)\u001b[38;5;241m.\u001b[39mstrip()) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    558\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m  \u001b[38;5;66;03m# If __file__ is not None the cause is unknown, so just re-raise.\u001b[39;00m\n\u001b[1;32m--> 560\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mdir\u001b[39m(\u001b[43m_C\u001b[49m):\n\u001b[0;32m    561\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m name\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBase\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m    562\u001b[0m         __all__\u001b[38;5;241m.\u001b[39mappend(name)\n",
      "\u001b[1;31mNameError\u001b[0m: name '_C' is not defined"
     ]
    }
   ],
   "source": [
    "from utils.DevConf import DevConf\n",
    "DEV_CONF = DevConf(device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\code\\python\\bert\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from utils.AttnBlocksConf import AttnBlocksConf\n",
    "from model.BertDecoder.SentiClassifier import SentiClassifier\n",
    "from model.CombinationModel import CombinationModel\n",
    "from utils.const import BlockType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\code\\python\\bert\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "mapper = SentiClassifier(6, AttnBlocksConf(768, 12, nKVHead=6), BlockType.CROSS)\n",
    "model = CombinationModel(nClass=6, decoder=mapper, devConf=DEV_CONF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-multilingual-cased\", cache_dir='./cache/tokenizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('data/archive/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.df.iloc[idx]['ABSTRACT']\n",
    "        label = torch.tensor([self.df.iloc[idx][i] for i in [\"Computer Science\",\"Physics\",\"Mathematics\",\"Statistics\",\"Quantitative Biology\",\"Quantitative Finance\"]])\n",
    "        return text, label\n",
    "        # return self.tokenizer(text, return_tensors='pt', padding='max_length', truncation=True, max_length=128), label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_fn(batch):\n",
    "    texts, labels = zip(*batch)\n",
    "    # print(texts)\n",
    "    return tokenizer(texts, return_tensors='pt', padding='max_length', truncation=True, max_length=512).to(device=DEV_CONF.device), torch.stack(labels).to(DEV_CONF.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MyDataset(train, tokenizer)\n",
    "\n",
    "datasize = len(train_dataset)\n",
    "splitIndex = int(datasize * 0.2)\n",
    "\n",
    "test_dataset = train_dataset[:splitIndex]\n",
    "train_dataset = train_dataset[splitIndex:]\n",
    "\n",
    "train_loader = DataLoader(train_dataset, collate_fn=collect_fn, batch_size=8, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, collate_fn=collect_fn, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-5\n",
    "epochs = 1\n",
    "loss_fn = nn.BCELoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(model, train_loader, loss_fn, optimizer, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        for i, (data, label) in enumerate(train_loader):\n",
    "            # print(data['input_ids'])\n",
    "            # break\n",
    "            optimizer.zero_grad()\n",
    "            output = model(**data)\n",
    "            loss = loss_fn(output, label.float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if i % 100 == 0:\n",
    "                print(f\"Epoch {epoch+1}/{epochs} - Batch {i+1}/{len(train_loader)} - Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1 - Batch 1/2622 - Loss: 0.9170064330101013\n",
      "Epoch 1/1 - Batch 101/2622 - Loss: 0.6943032741546631\n",
      "Epoch 1/1 - Batch 201/2622 - Loss: 0.69356369972229\n",
      "Epoch 1/1 - Batch 301/2622 - Loss: 0.6934887170791626\n",
      "Epoch 1/1 - Batch 401/2622 - Loss: 0.6930962800979614\n",
      "Epoch 1/1 - Batch 501/2622 - Loss: 0.6934904456138611\n",
      "Epoch 1/1 - Batch 601/2622 - Loss: 0.6934870481491089\n",
      "Epoch 1/1 - Batch 701/2622 - Loss: 0.6946806907653809\n",
      "Epoch 1/1 - Batch 801/2622 - Loss: 0.6845504641532898\n",
      "Epoch 1/1 - Batch 901/2622 - Loss: 0.6935328245162964\n",
      "Epoch 1/1 - Batch 1001/2622 - Loss: 0.6776870489120483\n",
      "Epoch 1/1 - Batch 1101/2622 - Loss: 0.6853147745132446\n",
      "Epoch 1/1 - Batch 1201/2622 - Loss: 0.6559020280838013\n",
      "Epoch 1/1 - Batch 1301/2622 - Loss: 0.6415303945541382\n",
      "Epoch 1/1 - Batch 1401/2622 - Loss: 0.6483358144760132\n",
      "Epoch 1/1 - Batch 1501/2622 - Loss: 0.671057403087616\n",
      "Epoch 1/1 - Batch 1601/2622 - Loss: 0.6766590476036072\n",
      "Epoch 1/1 - Batch 1701/2622 - Loss: 0.6568801999092102\n",
      "Epoch 1/1 - Batch 1801/2622 - Loss: 0.6692042350769043\n",
      "Epoch 1/1 - Batch 1901/2622 - Loss: 0.6584327816963196\n",
      "Epoch 1/1 - Batch 2001/2622 - Loss: 0.6550871133804321\n",
      "Epoch 1/1 - Batch 2101/2622 - Loss: 0.6625561118125916\n",
      "Epoch 1/1 - Batch 2201/2622 - Loss: 0.6841193437576294\n",
      "Epoch 1/1 - Batch 2301/2622 - Loss: 0.6589218378067017\n",
      "Epoch 1/1 - Batch 2401/2622 - Loss: 0.6654835939407349\n",
      "Epoch 1/1 - Batch 2501/2622 - Loss: 0.6462973952293396\n",
      "Epoch 1/1 - Batch 2601/2622 - Loss: 0.6424533128738403\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "train_fn(model, train_loader, loss_fn, optimizer, epochs)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CombinationModel(\n",
       "  (distilBert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): SentiClassifier(\n",
       "    (mapper): CACBlocks(\n",
       "      (_mha): ModuleList(\n",
       "        (0-5): 6 x MHABlock(\n",
       "          (_mha): Attention(\n",
       "            (qProj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (kvProj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (outProj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (ffn): Sequential(\n",
       "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (3): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (outNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (outProj): Linear(in_features=768, out_features=6, bias=True)\n",
       "  (activate): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 0, 1, 0, 0, 0]], device='cuda:0')\n",
      "tensor([1, 0, 0, 0, 0, 0])\n",
      "tensor([[False,  True, False,  True,  True,  True]])\n",
      "tensor(False)\n"
     ]
    }
   ],
   "source": [
    "id = 13\n",
    "sample = tokenizer(train_dataset[id][0], return_tensors='pt', padding='max_length', truncation=True, max_length=512).to(device=DEV_CONF.device)\n",
    "output = torch.where(model(**sample) > 0.2, 1, 0)\n",
    "print(output)\n",
    "print(train_dataset[id][1])\n",
    "ans = torch.eq(output.to(\"cpu\"), train_dataset[id][1])\n",
    "print(ans)\n",
    "print(torch.all(ans))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# acc = [0] * 6\n",
    "# testdata = train.sample(1000)\n",
    "# test_dataset = MyDataset(testdata, tokenizer)\n",
    "\n",
    "# for data in test_dataset:\n",
    "#     # print(data[1][0])\n",
    "#     sample = tokenizer(data[0], return_tensors='pt', padding='max_length', truncation=True, max_length=512).to(device=DEV_CONF.device)\n",
    "#     output = torch.where(model(**sample) > 0.2, 1, 0)\n",
    "#     ansList = torch.eq(output.squeeze().to(\"cpu\"), data[1])\n",
    "#     for i, ans in enumerate(ansList):\n",
    "#         # print(ans)\n",
    "#         if ans:\n",
    "#             acc[i] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_loader):\n",
    "    acc = [0] * 6\n",
    "    for i, (data, label) in enumerate(test_loader):\n",
    "        output = torch.where(model(**data) > 0.2, 1, 0)\n",
    "        ansList = torch.eq(output.to(\"cpu\"), label)\n",
    "        for i, ans in enumerate(ansList):\n",
    "            if ans:\n",
    "                acc[i] += 1\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = test(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[836, 919, 886, 770, 968, 992]\n"
     ]
    }
   ],
   "source": [
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
